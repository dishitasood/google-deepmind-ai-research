{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuulv9162h3v+kzpq1jNgy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dishitasood/google-deepmind-ai-research/blob/main/Preparing_a_Datset_for_Training_an_SLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "htUztQFcwWnC"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install \"git+https://github.com/google-deepmind/ai-foundations.git@main\"\n",
        "\n",
        "import re # Used for splitting strings on spaces.\n",
        "\n",
        "# Packages used.\n",
        "import pandas as pd # For reading the dataset.\n",
        "import textwrap # For adding linebreaks to paragraphs.\n",
        "\n",
        "# For providing feedback.\n",
        "from ai_foundations.feedback.course_1 import slm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading and Tokenization the dataset"
      ],
      "metadata": {
        "id": "NPzUYEI8wjgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "africa_galore = pd.read_json(\n",
        "    \"https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore.json\"\n",
        ")\n",
        "\n",
        "dataset = africa_galore['description']\n",
        "print(f\"Loaded Africa Galore dataset: {len(dataset)} paragraphs\")\n",
        "print(f\"\\nFirst Paragraph:\")\n",
        "print(textwrap.fill(dataset[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVGnMU3Twe3P",
        "outputId": "3908a3c1-beb1-4873-b9d7-c21ff75e1fed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded Africa Galore dataset: 232 paragraphs\n",
            "\n",
            "First Paragraph:\n",
            "The Lagos air was thick with humidity, but the energy in the club was\n",
            "electric. The band launched into a hypnotic Afrobeat groove, the drums\n",
            "pounding out a complex polyrhythm, the horns blaring a soaring melody,\n",
            "and the bass laying down a deep, funky foundation. A woman named Imani\n",
            "moved effortlessly to the music, her body swaying in time with the\n",
            "rhythm. The music seemed to flow through her, a powerful current of\n",
            "energy and joy. All around her, people were dancing, singing, and\n",
            "clapping, caught up in the infectious rhythm. The music was more than\n",
            "just entertainment; it was a celebration of life, a connection to\n",
            "their shared heritage, a vibrant expression of the soul of Lagos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def space_tokenize(text: str) -> list[str]:\n",
        "    \"\"\"Splits a string into a list of tokens.\n",
        "\n",
        "    Splits text on space.\n",
        "\n",
        "    Args:\n",
        "        text: The input text.\n",
        "\n",
        "    Returns:\n",
        "        A list of tokens. Returns empty list if text is empty or all spaces.\n",
        "    \"\"\"\n",
        "    # Use `re` package so that splitting on multiple spaces also works.\n",
        "    tokens = re.split(r\" +\", text)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "# Tokenize an example text with the `space_tokenize` function.\n",
        "space_tokenize(\"Kanga, a colorful printed cloth is more than just a fabric.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJsHkwp-xFmy",
        "outputId": "db91687d-772f-4fa3-de8d-5c18856e0bfc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Kanga,',\n",
              " 'a',\n",
              " 'colorful',\n",
              " 'printed',\n",
              " 'cloth',\n",
              " 'is',\n",
              " 'more',\n",
              " 'than',\n",
              " 'just',\n",
              " 'a',\n",
              " 'fabric.']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build a list of tokens in the dataset"
      ],
      "metadata": {
        "id": "jptc_jXyxOCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = []\n",
        "for paragraph in dataset:\n",
        "  for token in space_tokenize(paragraph):\n",
        "    tokens.append(token)\n",
        "print(\"The total number of tokens are: \", len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqfYuk1xxLY2",
        "outputId": "ff26140e-c418-4921-a698-7e9c37cb162e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total number of tokens are:  19065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the vocabulary"
      ],
      "metadata": {
        "id": "r64TcQM7xqF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocabulary(tokens: list[str]) -> list[str]:\n",
        "  vocabulary = list(set(tokens)) # there are no duplicate values in a set\n",
        "  return vocabulary"
      ],
      "metadata": {
        "id": "mhFAeb5Pxmhk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "slm.test_build_vocabulary(build_vocabulary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mz82038yAZb",
        "outputId": "90a07e42-e792-4dec-b259-09c36dcc5e9f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Nice! Your answer looks correct.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = build_vocabulary(tokens)\n",
        "\n",
        "vocabulary_size = len(vocabulary)\n",
        "\n",
        "print(\n",
        "    \"Total number of unique tokens in the Africa Galore dataset:\"\n",
        "    f\" {vocabulary_size:,}\"\n",
        ")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tocR1JB8yD3U",
        "outputId": "904810b4-fa53-4b2c-e0f6-0e447656c21b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of unique tokens in the Africa Galore dataset: 5,260\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary[:30]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9N2BOCyyDvP",
        "outputId": "d55872dd-5b06-4647-932a-14bda452fb8a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " 'Algeria,',\n",
              " 'zesty',\n",
              " \"'king\",\n",
              " 'values',\n",
              " 'Older',\n",
              " 'curries',\n",
              " 'fats.',\n",
              " 'juicy',\n",
              " 'congregate',\n",
              " 'As',\n",
              " 'Laila',\n",
              " 'men,',\n",
              " 'When',\n",
              " 'our',\n",
              " 'smell',\n",
              " 'mild,',\n",
              " 'news,',\n",
              " 'Senegalese',\n",
              " 'driest',\n",
              " 'side,',\n",
              " 'distant',\n",
              " 'liquid.',\n",
              " 'footsteps',\n",
              " 'furrowed',\n",
              " 'Its',\n",
              " '(79',\n",
              " 'properly,',\n",
              " 'rice,',\n",
              " 'multiple']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build token_to_index"
      ],
      "metadata": {
        "id": "G-rIqM2hyds9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_to_index = {}\n",
        "\n",
        "for index, token in enumerate(vocabulary):\n",
        "  token_to_index[token] = index"
      ],
      "metadata": {
        "id": "8U_jnSd9yUFb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_token = {}\n",
        "for token, index in token_to_index.items():\n",
        "    index_to_token[index] = token"
      ],
      "metadata": {
        "id": "w3YlnT7_y6vo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "slm.test_index_to_token(index_to_token, vocabulary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5DlsZU6yty3",
        "outputId": "5ad0a31b-c883-4c50-e003-e0091d1985a5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Nice! Your answer looks correct.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"token_to_index:\\n\")\n",
        "\n",
        "count = 0\n",
        "first_ten_indices = []\n",
        "for token, token_id in token_to_index.items():\n",
        "    print(f\"'{token}': {token_id}\")\n",
        "    first_ten_indices.append(token_id)\n",
        "    count += 1\n",
        "    if count == 10:\n",
        "        break\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "print(\"index_to_token:\\n\")\n",
        "for token_id in first_ten_indices:\n",
        "    print(f\"{token_id}: '{index_to_token[token_id]}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJiskTK6zDTN",
        "outputId": "3e8d63e3-ba7d-4fb0-be4d-d3ae890a6045"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token_to_index:\n",
            "\n",
            "'': 0\n",
            "'Algeria,': 1\n",
            "'zesty': 2\n",
            "''king': 3\n",
            "'values': 4\n",
            "'Older': 5\n",
            "'curries': 6\n",
            "'fats.': 7\n",
            "'juicy': 8\n",
            "'congregate': 9\n",
            "\n",
            "\n",
            "\n",
            "index_to_token:\n",
            "\n",
            "0: ''\n",
            "1: 'Algeria,'\n",
            "2: 'zesty'\n",
            "3: ''king'\n",
            "4: 'values'\n",
            "5: 'Older'\n",
            "6: 'curries'\n",
            "7: 'fats.'\n",
            "8: 'juicy'\n",
            "9: 'congregate'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encode and Decode Functions"
      ],
      "metadata": {
        "id": "qp_EkC1OzLrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def encode(text: str) -> list[int]:\n",
        "    \"\"\"Encodes a text sequence into a list of indices based on the vocabulary.\n",
        "\n",
        "    Args:\n",
        "        text: The input text to be encoded.\n",
        "\n",
        "    Returns:\n",
        "        A list of indices corresponding to the tokens in the input text.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert tokens into indices.\n",
        "    indices = []\n",
        "    for token in space_tokenize(text):\n",
        "        token_index = token_to_index.get(token)\n",
        "        indices.append(token_index)\n",
        "\n",
        "    return indices\n",
        "\n",
        "\n",
        "def decode(indices: int | list[int]) -> list[str]:\n",
        "    \"\"\"Decodes a list (or single index) of integers back into tokens.\n",
        "\n",
        "    Args:\n",
        "        indices: A single index or a list of indices to be decoded into tokens.\n",
        "\n",
        "    Returns:\n",
        "        str: A string of decoded tokens corresponding to the input indices.\n",
        "    \"\"\"\n",
        "\n",
        "    # If a single integer is passed, convert it into a list.\n",
        "    if isinstance(indices, int):\n",
        "        indices = [indices]\n",
        "\n",
        "    # Map indices to tokens.\n",
        "    tokens = []\n",
        "    for index in indices:\n",
        "        token = index_to_token.get(index)\n",
        "        tokens.append(token)\n",
        "\n",
        "    # Join the decoded tokens into a single string.\n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "cuPNBdCQzISx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = dataset[0]\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8II2FTwzUqp",
        "outputId": "bfe091c3-4112-4a4e-b688-54e9ee188f00"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Lagos air was thick with humidity, but the energy in the club was electric. The band launched into a hypnotic Afrobeat groove, the drums pounding out a complex polyrhythm, the horns blaring a soaring melody, and the bass laying down a deep, funky foundation. A woman named Imani moved effortlessly to the music, her body swaying in time with the rhythm. The music seemed to flow through her, a powerful current of energy and joy. All around her, people were dancing, singing, and clapping, caught up in the infectious rhythm. The music was more than just entertainment; it was a celebration of life, a connection to their shared heritage, a vibrant expression of the soul of Lagos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encode(text)[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOUq-YvxzXgL",
        "outputId": "285fae01-453b-4b67-de11-feb25a62c2b0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2289, 2873, 1198, 2852, 84, 4303, 183, 2199, 2415, 3629]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode(encode(text)[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vvJKLI0pzaFz",
        "outputId": "1bc02e32-811a-441e-dbc5-b3f709cdb7b2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Lagos air was thick with humidity, but the energy'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleWordTokenizer:\n",
        "    \"\"\"A simple word tokenizer that can be initialized with a corpus of texts\n",
        "       or using a provided vocabulary list.\n",
        "\n",
        "    The tokenizer splits the text sequence based on spaces,\n",
        "    using the `encode` method to convert the text into a sequence of indices\n",
        "    and the `decode` method to convert indices back into text.\n",
        "\n",
        "    Typical usage example:\n",
        "\n",
        "        corpus = \"Hello there!\"\n",
        "        tokenizer = SimpleWordTokenizer(corpus)\n",
        "        print(tokenizer.encode('Hello'))\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, corpus: list[str], vocabulary: list[str] | None = None):\n",
        "        \"\"\"Initializes the tokenizer with texts in corpus or with a vocabulary.\n",
        "\n",
        "        Args:\n",
        "            corpus: Input text dataset.\n",
        "            vocabulary: A pre-defined vocabulary. If None,\n",
        "                the vocabulary is automatically inferred from the texts.\n",
        "        \"\"\"\n",
        "\n",
        "        if vocabulary is None:\n",
        "            # Build the vocabulary from scratch.\n",
        "            if isinstance(corpus, str):\n",
        "                corpus = [corpus]\n",
        "\n",
        "            # Convert text sequence to tokens.\n",
        "            tokens = []\n",
        "            for text in corpus:\n",
        "                for token in self.space_tokenize(text):\n",
        "                    tokens.append(token)\n",
        "\n",
        "            # Create a vocabulary comprising of unique tokens.\n",
        "            self.vocabulary = self.build_vocabulary(tokens)\n",
        "\n",
        "        else:\n",
        "            self.vocabulary = vocabulary\n",
        "\n",
        "        # Size of vocabulary.\n",
        "        self.vocabulary_size = len(self.vocabulary)\n",
        "\n",
        "        # Create token-to-index and index-to-token mappings.\n",
        "        self.token_to_index = {}\n",
        "        self.index_to_token = {}\n",
        "        # Loop through all tokens in the vocabulary. enumerate automatically\n",
        "        # assigns a unique index to each token.\n",
        "        for index, token in enumerate(self.vocabulary):\n",
        "            self.token_to_index[token] = index\n",
        "            self.index_to_token[index] = token\n",
        "\n",
        "    def space_tokenize(self, text: str) -> list[str]:\n",
        "        \"\"\"Splits a given text on space into tokens.\n",
        "\n",
        "        Args:\n",
        "            text: Text to split on space.\n",
        "\n",
        "        Returns:\n",
        "            List of tokens after splitting `text`.\n",
        "        \"\"\"\n",
        "\n",
        "        # Use re.split such that multiple spaces are treated as a single\n",
        "        # separator.\n",
        "        return re.split(\" +\", text)\n",
        "\n",
        "    def join_text(self, text_list: list[str]) -> str:\n",
        "        \"\"\"Combines a list of tokens into a single string, with tokens separated\n",
        "           by spaces.\n",
        "\n",
        "        Args:\n",
        "            text_list: List of tokens to be joined.\n",
        "\n",
        "        Returns:\n",
        "            String with all tokens joined with a space.\n",
        "\n",
        "        \"\"\"\n",
        "        return \" \".join(text_list)\n",
        "\n",
        "    def build_vocabulary(self, tokens: list[str]) -> list[str]:\n",
        "        \"\"\"Create a vocabulary list from the list of tokens.\n",
        "\n",
        "        Args:\n",
        "            tokens: The list of tokens in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            List of unique tokens (vocabulary) in the dataset.\n",
        "        \"\"\"\n",
        "        return sorted(list(set(tokens)))\n",
        "\n",
        "    def encode(self, text: str) -> list[int]:\n",
        "        \"\"\"Encodes a text sequence into a list of indices.\n",
        "\n",
        "        Args:\n",
        "            text: The input text to be encoded.\n",
        "\n",
        "        Returns:\n",
        "            A list of indices corresponding to the tokens in the input text.\n",
        "        \"\"\"\n",
        "\n",
        "        # Convert tokens into indices.\n",
        "        indices = []\n",
        "        for token in self.space_tokenize(text):\n",
        "            token_index = self.token_to_index.get(token)\n",
        "            indices.append(token_index)\n",
        "\n",
        "        return indices\n",
        "\n",
        "    def decode(self, indices: int | list[int]) -> str:\n",
        "        \"\"\"Decodes a list (or single index) of integers back into tokens.\n",
        "\n",
        "        Args:\n",
        "            indices: A single index or a list of indices to be decoded into\n",
        "                tokens.\n",
        "\n",
        "        Returns:\n",
        "            str: A string of decoded tokens corresponding to the input indices.\n",
        "        \"\"\"\n",
        "\n",
        "        # If a single integer is passed, convert it into a list.\n",
        "        if isinstance(indices, int):\n",
        "            indices = [indices]\n",
        "\n",
        "        # Map indices to tokens.\n",
        "        tokens = []\n",
        "        for index in indices:\n",
        "            token = self.index_to_token.get(index)\n",
        "            tokens.append(token)\n",
        "\n",
        "        # Join the decoded tokens into a single string.\n",
        "        return self.join_text(tokens)"
      ],
      "metadata": {
        "id": "yDiV0ti0zcRK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleWordTokenizer(dataset)\n",
        "slm.test_simple_word_tokenizer(tokenizer, vocabulary, dataset)"
      ],
      "metadata": {
        "id": "eNHoLsnPzm2K",
        "outputId": "3fea9143-4741-490e-efae-91511f0a5af4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Nice! The tokenizer seems to be working correctly.\n"
          ]
        }
      ]
    }
  ]
}